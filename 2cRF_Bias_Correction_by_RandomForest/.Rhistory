} else {
# Delete everything in the 'temp' folder if it exists
temp_files <- list.files("temp", full.names = TRUE)
file.remove(temp_files)
}
all_objects <- ls()
data_objects <- all_objects[sapply(all_objects, function(x) !is.function(get(x)))]
# Filter by size (greater than 1e5 bytes)
large_data_objects <- data_objects[sapply(data_objects, function(x) object.size(get(x)) > 1e5)]
objects_to_save <- setdiff(large_data_objects, c("Matrix_training_high", "DataInt_training_high"))
for (obj in objects_to_save) {
save(list = obj, file = paste0("temp/", obj, ".RData"))
rm(list = obj)
}
# Inspect data types before training
cat("Inspecting data types before training...\n")
str(Matrix_training_high)
# Setup parallel processing if not already setup may need to revise more
try(stopAllClusters(), silent = TRUE)
num_cores <- floor(detectCores() * 3 / 4) # using 3/4 of all cores
cl <- makeCluster(num_cores)
try(registerDoParallel(cl), silent = TRUE)
load("clf_rf_high_i0_seed2024.rds")
1+1=
load("clf_rf_high_i0_seed2024.rds")
readRDS("clf_rf_high_i0_seed2024.rds")
library(readxl) #load xlsx
library(purrr) #load xlsx by iterations
library(openxlsx) #write xlsx
library(magrittr) #pipe
library(tidyr) #df cleaning
library(plyr)  #df splitting and combining
library(dplyr) #df data manipulation
library(caret) #training
library(randomForest) #rf related plot
library(doParallel) # parallel computation
library(data.table) # handle large data without using too much RAM
library(tidyverse) # ggplot2, dplyr and more
library(ggplot2)
library(writexl)
library(gridExtra) # enable displaying multiple ggplot2 in one page
directory <- getwd()
source("2c_Functions.R") # custom functions
# seed_i is the random seed used to separate 80% training and 20% test data by sample
seed_i <- 2024
# total number of model training reruns to check if the result is robust, in each rerun, the used seed will +1 from the previous
num_rerun <- 1
# inner_cv is the within training cross-validation fold, 3 is chose to speed up training
inner_cv <- 3
# Recommended ntree for this data size
ntree <- 2000  # Adjust as necessary, 2000 is a good one for this study but be careful to not over RAM if your computer is not good. You may try differnt values for optimization
mtry_fixed <- 5 # A common starting point for mtry(tree depth) in RF
# Test initial high intensity model accuracy
predictions_high_training <- predict(clf_rf_high, newdata = Matrix_training_high)
clf_rf_high<-readRDS("clf_rf_high_i0_seed2024.rds")
# Test initial high intensity model accuracy
predictions_high_training <- predict(clf_rf_high, newdata = Matrix_training_high)
input_dir <- file.path(directory, "input")
if (!dir.exists(input_dir)) {
dir.create(input_dir)
}
file_path_df <- file.path(input_dir, "df_2b.RData")
file_path_high <- file.path(input_dir, "df_by_mn_high_2b.RData")
file_path_low  <- file.path(input_dir, "df_by_mn_low_2b.RData")
file_path_slice  <- file.path(input_dir, "df_by_slice_2b.RData")
load(file = file_path_df)
load(file = file_path_high)
load(file = file_path_low)
load(file = file_path_slice)
# Define log file path ans start logging
write("", file = "output_log.txt")
log_file <- "output_log.txt"
sink(log_file, append = TRUE)
cat("------------------------------------------------------\n")
cat("Start running trial", i, "out of total trials", num_rerun, "by using random seed", seed_i, "\n")
i=1
cat("------------------------------------------------------\n")
cat("Start running trial", i, "out of total trials", num_rerun, "by using random seed", seed_i, "\n")
cat("The used inner cross-validation fold is", inner_cv, "\n")
cat("The used number of trees in random forest training is", ntree, "\n")
cat("The used depth of trees in random forest training is", mtry_fixed, "\n")
cat("------------------------------------------------------\n")
# Randomize samples into 80% training and 20% test based on sampple name and random seed
df$sample <- as.character(df$sample)
set.seed(seed_i)
unique_values <- unique(df$sample)
index <- seq(5, length(unique_values), by=5)
selected_values <- unique_values[index]
training_df <- df %>% filter(!sample %in% selected_values)
test_df <- df %>% filter(sample %in% selected_values)
training_df_by_mn_high <- df_by_mn_high %>% filter(!sample %in% selected_values)
training_df_by_mn_low <- df_by_mn_low %>% filter(!sample %in% selected_values)
test_df_by_mn_high <- df_by_mn_high %>% filter(sample %in% selected_values)
test_df_by_mn_low <- df_by_mn_low %>% filter(sample %in% selected_values)
# Calculate and print the statistics for M1 M2 RIA
cat("Uncorrected Mean Absolute Error of M1 RIA (%):", mean(abs(100 * test_df$delta_m1_percentage_sub_b), na.rm = TRUE), "\n")
cat("Uncorrected Median Absolute Error of M1 RIA (%):", median(abs(100 * test_df$delta_m1_percentage_sub_b), na.rm = TRUE), "\n")
cat("Uncorrected sd of M1 RIA (%):", sd(100 * test_df$delta_m1_percentage_sub_b, na.rm = TRUE), "\n")
cat("Uncorrected Mean Absolute Error of M2 RIA (%):", mean(abs(100 * test_df$delta_m2_percentage_sub_b), na.rm = TRUE), "\n")
cat("Uncorrected Median Absolute Error of M2 RIA (%):", median(abs(100 * test_df$delta_m2_percentage_sub_b), na.rm = TRUE), "\n")
cat("Uncorrected sd of M2 RIA (%):", sd(100 * test_df$delta_m2_percentage_sub_b, na.rm = TRUE), "\n")
# First convert the format to subset, then subset the needed non-numerical columns into new data.tables
training_df_by_mn_high <- as.data.table(training_df_by_mn_high)
training_df_by_mn_low <- as.data.table(training_df_by_mn_low)
test_df_by_mn_high <- as.data.table(test_df_by_mn_high)
test_df_by_mn_low <- as.data.table(test_df_by_mn_low)
training_high_info <- training_df_by_mn_high[, .(compound, sample, formula, slice, intensity_predicted, intensity_smoothed_predicted, ratio, intensity_minus_bg, intensity_minus_bg_smoothed)]
training_low_info <- training_df_by_mn_low[, .(compound, sample, formula, slice, intensity_predicted, intensity_smoothed_predicted, ratio, intensity_minus_bg, intensity_minus_bg_smoothed)]
test_high_info <- test_df_by_mn_high[, .(compound, sample, formula, slice, intensity_predicted, intensity_smoothed_predicted, ratio, intensity_minus_bg, intensity_minus_bg_smoothed)]
test_low_info <- test_df_by_mn_low[, .(compound, sample, formula, slice, intensity_predicted, intensity_smoothed_predicted, ratio, intensity_minus_bg, intensity_minus_bg_smoothed)]
# Remove unused non-numerical columns and force set rest everytying are all numericals
training_df_by_mn_high[, `:=`(compound = NULL, sample = NULL, formula = NULL, ratio = NULL, intensity_predicted = NULL, intensity_smoothed_predicted = NULL,  intensity_minus_bg = NULL, intensity_minus_bg_smoothed = NULL)]
training_df_by_mn_low[, `:=`(compound = NULL, sample = NULL, formula = NULL, ratio = NULL, intensity_predicted = NULL, intensity_smoothed_predicted = NULL,  intensity_minus_bg = NULL, intensity_minus_bg_smoothed = NULL)]
test_df_by_mn_high[, `:=`(compound = NULL, sample = NULL, formula = NULL, ratio = NULL, intensity_predicted = NULL, intensity_smoothed_predicted = NULL,  intensity_minus_bg = NULL, intensity_minus_bg_smoothed = NULL)]
test_df_by_mn_low[, `:=`(compound = NULL, sample = NULL, formula = NULL, ratio = NULL, intensity_predicted = NULL, intensity_smoothed_predicted = NULL,  intensity_minus_bg = NULL, intensity_minus_bg_smoothed = NULL)]
training_df_by_mn_high[, `:=`(rt_left = NULL, rt_peak = NULL, rt_right = NULL, slice = NULL, total_slices = NULL, slice_peak_height = NULL, mplus1_intensity_smoothed = NULL, highest_original_intensity = NULL, rt_width = NULL)]
training_df_by_mn_low[, `:=`(rt_left = NULL, rt_peak = NULL, rt_right = NULL, slice = NULL, total_slices = NULL, slice_peak_height = NULL, mplus1_intensity_smoothed = NULL, highest_original_intensity = NULL, rt_width = NULL)]
test_df_by_mn_high[, `:=`(rt_left = NULL, rt_peak = NULL, rt_right = NULL, slice = NULL, total_slices = NULL, slice_peak_height = NULL, mplus1_intensity_smoothed = NULL, highest_original_intensity = NULL, rt_width = NULL)]
test_df_by_mn_low[, `:=`(rt_left = NULL, rt_peak = NULL, rt_right = NULL, slice = NULL, total_slices = NULL, slice_peak_height = NULL, mplus1_intensity_smoothed = NULL, highest_original_intensity = NULL, rt_width = NULL)]
training_df_by_mn_high <- training_df_by_mn_high[, lapply(.SD, as.numeric)]
training_df_by_mn_low <- training_df_by_mn_low[, lapply(.SD, as.numeric)]
test_df_by_mn_high <- test_df_by_mn_high[, lapply(.SD, as.numeric)]
test_df_by_mn_low <- test_df_by_mn_low[, lapply(.SD, as.numeric)]
#########################################
# For this study, we also remove the columns "slice_diff_to_m0peak" and "Si" as after preliminary inspectation, slice_diff have nothing to do with bias and no metabolites contain "Si"
training_df_by_mn_high[, c("slice_diff_to_m0peak", "Si") := NULL]
training_df_by_mn_low[, c("slice_diff_to_m0peak", "Si") := NULL]
test_df_by_mn_high[, c("slice_diff_to_m0peak", "Si") := NULL]
test_df_by_mn_low[, c("slice_diff_to_m0peak", "Si") := NULL]
########################################
# Convert data.tables to matrix for latering inputing into training
Matrix_training_high <- as.matrix(training_df_by_mn_high)
Matrix_training_low <- as.matrix(training_df_by_mn_low)
Matrix_test_high <- as.matrix(test_df_by_mn_high)
Matrix_test_low <- as.matrix(test_df_by_mn_low)
# Extract intensity_predicted column from training_high_info
DataInt_training_high <- training_high_info$intensity_minus_bg_smoothed / training_high_info$intensity_smoothed_predicted
DataInt_test_high <- test_high_info$intensity_minus_bg_smoothed / test_high_info$intensity_smoothed_predicted
lobstr::obj_size()
objs <- ls(envir = .GlobalEnv)
sizes <- sapply(objs, function(x) object.size(get(x)))
sizes_df <- data.frame(Object = objs, Size_MB = round(sizes / 1024^2, 2))
sizes_df[order(-sizes_df$Size_MB), ]  # Show largest first
View(sizes_df)
rm(clf_rf_high)
####################################
#####################################
library(readxl) #load xlsx
library(purrr) #load xlsx by iterations
library(openxlsx) #write xlsx
library(magrittr) #pipe
library(tidyr) #df cleaning
library(plyr)  #df splitting and combining
library(dplyr) #df data manipulation
library(caret) #training
library(randomForest) #rf related plot
library(doParallel) # parallel computation
library(data.table) # handle large data without using too much RAM
library(tidyverse) # ggplot2, dplyr and more
library(ggplot2)
library(writexl)
library(gridExtra) # enable displaying multiple ggplot2 in one page
directory <- getwd()
source("2c_Functions.R") # custom functions
# seed_i is the random seed used to separate 80% training and 20% test data by sample
seed_i <- 2024
# total number of model training reruns to check if the result is robust, in each rerun, the used seed will +1 from the previous
num_rerun <- 1
# inner_cv is the within training cross-validation fold, 3 is chose to speed up training
inner_cv <- 3
# Recommended ntree for this data size
ntree <- 2000  # Adjust as necessary, 2000 is a good one for this study but be careful to not over RAM if your computer is not good. You may try differnt values for optimization
mtry_fixed <- 5 # A common starting point for mtry(tree depth) in RF
####################################
#####################################
input_dir <- file.path(directory, "input")
if (!dir.exists(input_dir)) {
dir.create(input_dir)
}
file_path_df <- file.path(input_dir, "df_2b.RData")
file_path_high <- file.path(input_dir, "df_by_mn_high_2b.RData")
file_path_low  <- file.path(input_dir, "df_by_mn_low_2b.RData")
file_path_slice  <- file.path(input_dir, "df_by_slice_2b.RData")
load(file = file_path_df)
load(file = file_path_high)
load(file = file_path_low)
load(file = file_path_slice)
# Define log file path ans start logging
write("", file = "output_log.txt")
log_file <- "output_log.txt"
sink(log_file, append = TRUE)
#######################################
######################################
for (i in 1:num_rerun)
{
cat("------------------------------------------------------\n")
cat("Start running trial", i, "out of total trials", num_rerun, "by using random seed", seed_i, "\n")
cat("The used inner cross-validation fold is", inner_cv, "\n")
cat("The used number of trees in random forest training is", ntree, "\n")
cat("The used depth of trees in random forest training is", mtry_fixed, "\n")
cat("------------------------------------------------------\n")
# Randomize samples into 80% training and 20% test based on sampple name and random seed
df$sample <- as.character(df$sample)
set.seed(seed_i)
unique_values <- unique(df$sample)
index <- seq(5, length(unique_values), by=5)
selected_values <- unique_values[index]
training_df <- df %>% filter(!sample %in% selected_values)
test_df <- df %>% filter(sample %in% selected_values)
training_df_by_mn_high <- df_by_mn_high %>% filter(!sample %in% selected_values)
training_df_by_mn_low <- df_by_mn_low %>% filter(!sample %in% selected_values)
test_df_by_mn_high <- df_by_mn_high %>% filter(sample %in% selected_values)
test_df_by_mn_low <- df_by_mn_low %>% filter(sample %in% selected_values)
# Calculate and print the statistics for M1 M2 RIA
cat("Uncorrected Mean Absolute Error of M1 RIA (%):", mean(abs(100 * test_df$delta_m1_percentage_sub_b), na.rm = TRUE), "\n")
cat("Uncorrected Median Absolute Error of M1 RIA (%):", median(abs(100 * test_df$delta_m1_percentage_sub_b), na.rm = TRUE), "\n")
cat("Uncorrected sd of M1 RIA (%):", sd(100 * test_df$delta_m1_percentage_sub_b, na.rm = TRUE), "\n")
cat("Uncorrected Mean Absolute Error of M2 RIA (%):", mean(abs(100 * test_df$delta_m2_percentage_sub_b), na.rm = TRUE), "\n")
cat("Uncorrected Median Absolute Error of M2 RIA (%):", median(abs(100 * test_df$delta_m2_percentage_sub_b), na.rm = TRUE), "\n")
cat("Uncorrected sd of M2 RIA (%):", sd(100 * test_df$delta_m2_percentage_sub_b, na.rm = TRUE), "\n")
# First convert the format to subset, then subset the needed non-numerical columns into new data.tables
training_df_by_mn_high <- as.data.table(training_df_by_mn_high)
training_df_by_mn_low <- as.data.table(training_df_by_mn_low)
test_df_by_mn_high <- as.data.table(test_df_by_mn_high)
test_df_by_mn_low <- as.data.table(test_df_by_mn_low)
training_high_info <- training_df_by_mn_high[, .(compound, sample, formula, slice, intensity_predicted, intensity_smoothed_predicted, ratio, intensity_minus_bg, intensity_minus_bg_smoothed)]
training_low_info <- training_df_by_mn_low[, .(compound, sample, formula, slice, intensity_predicted, intensity_smoothed_predicted, ratio, intensity_minus_bg, intensity_minus_bg_smoothed)]
test_high_info <- test_df_by_mn_high[, .(compound, sample, formula, slice, intensity_predicted, intensity_smoothed_predicted, ratio, intensity_minus_bg, intensity_minus_bg_smoothed)]
test_low_info <- test_df_by_mn_low[, .(compound, sample, formula, slice, intensity_predicted, intensity_smoothed_predicted, ratio, intensity_minus_bg, intensity_minus_bg_smoothed)]
# Remove unused non-numerical columns and force set rest everytying are all numericals
training_df_by_mn_high[, `:=`(compound = NULL, sample = NULL, formula = NULL, ratio = NULL, intensity_predicted = NULL, intensity_smoothed_predicted = NULL,  intensity_minus_bg = NULL, intensity_minus_bg_smoothed = NULL)]
training_df_by_mn_low[, `:=`(compound = NULL, sample = NULL, formula = NULL, ratio = NULL, intensity_predicted = NULL, intensity_smoothed_predicted = NULL,  intensity_minus_bg = NULL, intensity_minus_bg_smoothed = NULL)]
test_df_by_mn_high[, `:=`(compound = NULL, sample = NULL, formula = NULL, ratio = NULL, intensity_predicted = NULL, intensity_smoothed_predicted = NULL,  intensity_minus_bg = NULL, intensity_minus_bg_smoothed = NULL)]
test_df_by_mn_low[, `:=`(compound = NULL, sample = NULL, formula = NULL, ratio = NULL, intensity_predicted = NULL, intensity_smoothed_predicted = NULL,  intensity_minus_bg = NULL, intensity_minus_bg_smoothed = NULL)]
training_df_by_mn_high[, `:=`(rt_left = NULL, rt_peak = NULL, rt_right = NULL, slice = NULL, total_slices = NULL, slice_peak_height = NULL, mplus1_intensity_smoothed = NULL, highest_original_intensity = NULL, rt_width = NULL)]
training_df_by_mn_low[, `:=`(rt_left = NULL, rt_peak = NULL, rt_right = NULL, slice = NULL, total_slices = NULL, slice_peak_height = NULL, mplus1_intensity_smoothed = NULL, highest_original_intensity = NULL, rt_width = NULL)]
test_df_by_mn_high[, `:=`(rt_left = NULL, rt_peak = NULL, rt_right = NULL, slice = NULL, total_slices = NULL, slice_peak_height = NULL, mplus1_intensity_smoothed = NULL, highest_original_intensity = NULL, rt_width = NULL)]
test_df_by_mn_low[, `:=`(rt_left = NULL, rt_peak = NULL, rt_right = NULL, slice = NULL, total_slices = NULL, slice_peak_height = NULL, mplus1_intensity_smoothed = NULL, highest_original_intensity = NULL, rt_width = NULL)]
training_df_by_mn_high <- training_df_by_mn_high[, lapply(.SD, as.numeric)]
training_df_by_mn_low <- training_df_by_mn_low[, lapply(.SD, as.numeric)]
test_df_by_mn_high <- test_df_by_mn_high[, lapply(.SD, as.numeric)]
test_df_by_mn_low <- test_df_by_mn_low[, lapply(.SD, as.numeric)]
#########################################
# For this study, we also remove the columns "slice_diff_to_m0peak" and "Si" as after preliminary inspectation, slice_diff have nothing to do with bias and no metabolites contain "Si"
training_df_by_mn_high[, c("slice_diff_to_m0peak", "Si") := NULL]
training_df_by_mn_low[, c("slice_diff_to_m0peak", "Si") := NULL]
test_df_by_mn_high[, c("slice_diff_to_m0peak", "Si") := NULL]
test_df_by_mn_low[, c("slice_diff_to_m0peak", "Si") := NULL]
########################################
# Convert data.tables to matrix for latering inputing into training
Matrix_training_high <- as.matrix(training_df_by_mn_high)
Matrix_training_low <- as.matrix(training_df_by_mn_low)
Matrix_test_high <- as.matrix(test_df_by_mn_high)
Matrix_test_low <- as.matrix(test_df_by_mn_low)
# Extract intensity_predicted column from training_high_info
DataInt_training_high <- training_high_info$intensity_minus_bg_smoothed / training_high_info$intensity_smoothed_predicted
DataInt_test_high <- test_high_info$intensity_minus_bg_smoothed / test_high_info$intensity_smoothed_predicted
# Inspect data types before training
cat("Inspecting data types before training...\n")
str(Matrix_training_high)
# Setup parallel processing if not already setup may need to revise more
try(stopAllClusters(), silent = TRUE)
num_cores <- floor(detectCores() * 3 / 4) # using 3/4 of all cores
cl <- makeCluster(num_cores)
try(registerDoParallel(cl), silent = TRUE)
# Use tryCatch to ensure cluster is stopped in case of an error
tryCatch({
# Set up cross-validation control
rfControl <- trainControl(method = "cv", number = inner_cv, verboseIter = TRUE, summaryFunction = mse_summary)
# Initial training on high intensity data
options(expressions = 100000)  # Set to a higher value than default to allow for more iterations
cat("Starting initial training on high-intensity data...\n")
set.seed(seed_i)
clf_rf_high <- train(x = Matrix_training_high,
y = DataInt_training_high,
method = "rf",
metric = "MSE",
trControl = rfControl,
ntree = ntree,
tuneGrid = data.frame(mtry = mtry_fixed))
cat("Initial training completed.\n")
# Save the initial model
saveRDS(clf_rf_high, file = paste0("clf_rf_high_i0_seed", seed_i, ".rds"))
# Extract and print importance
importance_high <- as.data.frame(importance(clf_rf_high$finalModel))
print(importance_high)
}, finally = {
# Ensure the cluster is stopped, and temp file moved back after the process
try(stopAllClusters(), silent = TRUE)
files_to_load <- list.files("temp", full.names = TRUE)
for (file in files_to_load) {
load(file)
}
# Delete everything in the 'temp' folder
file.remove(files_to_load)
})
# Test initial high intensity model accuracy
predictions_high_training <- predict(clf_rf_high, newdata = Matrix_training_high)
predictions_high_test <- predict(clf_rf_high, newdata = Matrix_test_high)
cat("Training Performance (high-intensity data) of the iteration 0 model trained only by high-intensity data:\n")
cat("Training Mean APE Error:", mean(abs(100 * ((training_high_info$intensity_minus_bg_smoothed / predictions_high_training) - training_high_info$intensity_smoothed_predicted) / training_high_info$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Training Median APE Error:", median(abs(100 * ((training_high_info$intensity_minus_bg_smoothed / predictions_high_training) - training_high_info$intensity_smoothed_predicted) / training_high_info$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Test Mean APE Error:", mean(abs(100 * ((test_high_info$intensity_minus_bg_smoothed / predictions_high_test) - test_high_info$intensity_smoothed_predicted) / test_high_info$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Test Median APE Error:", median(abs(100 * ((test_high_info$intensity_minus_bg_smoothed / predictions_high_test) - test_high_info$intensity_smoothed_predicted) / test_high_info$intensity_smoothed_predicted), na.rm = TRUE), "\n")
p_high_training_Int <- training_high_info$intensity_minus_bg_smoothed/predictions_high_training
p_high_test_Int <- test_high_info$intensity_minus_bg_smoothed/predictions_high_test
# Identify and print "super off" values (e.g., APE > 100%)
training_ape <- abs(100 * ((training_high_info$intensity_minus_bg_smoothed / predictions_high_training) - training_high_info$intensity_smoothed_predicted) / training_high_info$intensity_smoothed_predicted)
super_off_training <- which(training_ape > 50)
cat("Super Off Training Values (APE > 50%):\n")
cat(length(super_off_training), "out of", length(training_ape), "training Mn Signals are > 50% off.\n")
# Identify and print "super off" values (e.g., APE > 100%)
test_ape <- abs(100 * ((test_high_info$intensity_minus_bg_smoothed / predictions_high_test) - test_high_info$intensity_smoothed_predicted) / test_high_info$intensity_smoothed_predicted)
super_off_test <- which(test_ape > 50)
cat("Super Off Test Values (APE > 50%):\n")
cat(length(super_off_test), "out of", length(test_ape), "test Mn Signals are > 50% off.\n")
# For low intensity data below the cutoff, the initial clf_rf_high modell is used to predict their true intensity
result_training <- process_data(Matrix_training_low, training_low_info, clf_rf_high)
Matrix_training_low_m0 <- result_training$Matrix_m0
Matrix_training_low_m1 <- result_training$Matrix_m1
Matrix_training_low_m2 <- result_training$Matrix_m2
training_low_info_m0 <- result_training$info_m0
training_low_info_m1 <- result_training$info_m1
training_low_info_m2 <- result_training$info_m2
training_low_info_pseudoL <- result_training$info_pseudoL
result_test <- process_data(Matrix_test_low, test_low_info, clf_rf_high)
Matrix_test_low_m0 <- result_test$Matrix_m0
Matrix_test_low_m1 <- result_test$Matrix_m1
Matrix_test_low_m2 <- result_test$Matrix_m2
test_low_info_m0 <- result_test$info_m0
test_low_info_m1 <- result_test$info_m1
test_low_info_m2 <- result_test$info_m2
test_low_info_pseudoL <- result_test$info_pseudoL
# Testing clf_rf_high on low data
predictions_low_training <- predict(clf_rf_high, newdata = Matrix_training_low)
predictions_low_test <- predict(clf_rf_high, newdata = Matrix_test_low)
cat("Training Performance (low-intensity data) of the iteration 0 model trained only by high-intensity data:\n")
cat("Training Mean APE Error:", mean(abs(100 * ((training_low_info_pseudoL$intensity_minus_bg_smoothed / predictions_low_training) - training_low_info_pseudoL$intensity_smoothed_predicted) / training_low_info_pseudoL$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Training Median APE Error:", median(abs(100 * ((training_low_info_pseudoL$intensity_minus_bg_smoothed / predictions_low_training) - training_low_info_pseudoL$intensity_smoothed_predicted) / training_low_info_pseudoL$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Test Mean APE Error:", mean(abs(100 * ((test_low_info_pseudoL$intensity_minus_bg_smoothed / predictions_low_test) - test_low_info_pseudoL$intensity_smoothed_predicted) / test_low_info_pseudoL$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Test Median APE Error:", median(abs(100 * ((test_low_info_pseudoL$intensity_minus_bg_smoothed / predictions_low_test) - test_low_info_pseudoL$intensity_smoothed_predicted) / test_low_info_pseudoL$intensity_smoothed_predicted), na.rm = TRUE), "\n")
p_low_training_Int <- training_low_info_pseudoL$intensity_minus_bg_smoothed/predictions_low_training
p_low_test_Int <- test_low_info_pseudoL$intensity_minus_bg_smoothed/predictions_low_test
training_ape <- abs(100 * ((training_low_info_pseudoL$intensity_minus_bg_smoothed / predictions_low_training) - training_low_info_pseudoL$intensity_smoothed_predicted) / training_low_info_pseudoL$intensity_smoothed_predicted)
super_off_training <- which(training_ape > 50)
cat("Super Off Training Values (APE > 50%):\n")
cat(length(super_off_training), "out of", length(training_ape), "training Mn Signals are > 50% off.\n")
test_ape <- abs(100 * ((test_low_info_pseudoL$intensity_minus_bg_smoothed / predictions_low_test) - test_low_info_pseudoL$intensity_smoothed_predicted) / test_low_info_pseudoL$intensity_smoothed_predicted)
super_off_test <- which(test_ape > 50)
cat("Super Off Test Values (APE > 50%):\n")
cat(length(super_off_test), "out of", length(test_ape), "test Mn Signals are > 50% off.\n")
# Then, the pesudo labled low intenisty data will be added to the high-intensity data for retraining the model
Matrix_training_all <- rbind(Matrix_training_high, Matrix_training_low)
training_all_info <- rbind(training_high_info, training_low_info_pseudoL)
DataInt_training_all <- training_all_info$intensity_minus_bg_smoothed / training_all_info$intensity_smoothed_predicted
Matrix_training_all <- as.matrix(Matrix_training_all)
Matrix_test_all <- rbind(Matrix_test_high, Matrix_test_low)
test_all_info <- rbind(test_high_info, test_low_info_pseudoL)
DataInt_test_all <- test_all_info$intensity_minus_bg_smoothed / test_all_info$intensity_smoothed_predicted
Matrix_test_all <- as.matrix(Matrix_test_all)
predictions_all_training <- predict(clf_rf_high, newdata = Matrix_training_all)
predictions_all_test <- predict(clf_rf_high, newdata = Matrix_test_all)
cat("Training Performance (high and low intensity data pooled) of the iteration 0 model trained only by high-intensity data:\n")
cat("Training Mean APE Error:", mean(abs(100 * ((training_all_info$intensity_minus_bg_smoothed/predictions_all_training) - training_all_info$intensity_smoothed_predicted) / training_all_info$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Training Median APE Error:", median(abs(100 * ((training_all_info$intensity_minus_bg_smoothed/predictions_all_training) - training_all_info$intensity_smoothed_predicted) / training_all_info$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Test Mean APE Error:", mean(abs(100 * ((test_all_info$intensity_minus_bg_smoothed/predictions_all_test) - test_all_info$intensity_smoothed_predicted) / test_all_info$intensity_smoothed_predicted), na.rm = TRUE), "\n")
cat("Test Median APE Error:", median(abs(100 * ((test_all_info$intensity_minus_bg_smoothed/predictions_all_test) - test_all_info$intensity_smoothed_predicted) / test_all_info$intensity_smoothed_predicted), na.rm = TRUE), "\n")
training_ape <- abs(100 * ((training_all_info$intensity_minus_bg_smoothed / predictions_all_training) - training_all_info$intensity_smoothed_predicted) / training_all_info$intensity_smoothed_predicted)
# Identify and print "super off" values (e.g., APE > 100%)
super_off_training <- which(training_ape > 50)
cat("Super Off Training Values (APE > 50%):\n")
cat(length(super_off_training), "out of", length(training_ape), "training Mn Signals are > 50% off.\n")
test_ape <- abs(100 * ((test_all_info$intensity_minus_bg_smoothed / predictions_all_test) - test_all_info$intensity_smoothed_predicted) / test_all_info$intensity_smoothed_predicted)
super_off_test <- which(test_ape > 50)
cat("Super Off Test Values (APE > 50%):\n")
cat(length(super_off_test), "out of", length(test_ape), "test Mn Signals are > 50% off.\n")
rm(clf_rf_high) # remove clf_rf_high to save RAM, if needed can use readRDS to load back manually
try(stopAllClusters(), silent = TRUE)
num_cores <- floor(detectCores() * 3 / 4) # using 3/4 of all cores
cl <- makeCluster(num_cores)
try(registerDoParallel(cl), silent = TRUE)
# Inspect data types before training
cat("Inspecting data types before training...\n")
str(Matrix_training_all)
source("C:/Users/zhenw/Marc Hellerstein Lab/Orbitrap_Paper/2cRF_Bias_Correction_by_RandomForest/2cRF_Bias_Correction_by_RandomForest_20250927_bugfixed.R", echo=TRUE)
library(readxl) #load xlsx
library(purrr) #load xlsx by iterations
library(openxlsx) #write xlsx
library(magrittr) #pipe
library(tidyr) #df cleaning
library(plyr)  #df splitting and combining
library(dplyr) #df data manipulation
library(caret) #training
library(randomForest) #rf related plot
library(doParallel) # parallel computation
library(data.table) # handle large data without using too much RAM
library(tidyverse) # ggplot2, dplyr and more
library(ggplot2)
library(writexl)
library(gridExtra) # enable displaying multiple ggplot2 in one page
directory <- getwd()
source("2c_Functions.R") # custom functions
# seed_i is the random seed used to separate 80% training and 20% test data by sample
seed_i <- 2024
# total number of model training reruns to check if the result is robust, in each rerun, the used seed will +1 from the previous
num_rerun <- 1
# inner_cv is the within training cross-validation fold, 3 is chose to speed up training
inner_cv <- 3
# Recommended ntree for this data size
ntree <- 2000  # Adjust as necessary, 2000 is a good one for this study but be careful to not over RAM if your computer is not good. You may try differnt values for optimization
mtry_fixed <- 5 # A common starting point for mtry(tree depth) in RF
rm(clf_rf_high)
rm(clf_rf_high) # remove clf_rf_high to save RAM, if needed can use readRDS to load back manually
try(stopAllClusters(), silent = TRUE)
num_cores <- floor(detectCores() * 3 / 4) # using 3/4 of all cores
cl <- makeCluster(num_cores)
try(registerDoParallel(cl), silent = TRUE)
# Inspect data types before training
cat("Inspecting data types before training...\n")
str(Matrix_training_all)
# Use tryCatch to ensure cluster is stopped in case of an error
tryCatch({
# Set up cross-validation control
rfControl <- trainControl(method = "cv", number = inner_cv, verboseIter = TRUE, summaryFunction = mse_summary)
# Initial training on high intensity data
options(expressions = 100000)  # Set to a higher value than default to allow for more iterations
cat("Starting initial training on all-intensity data...\n")
set.seed(seed_i)
clf_rf_all <- train(x = Matrix_training_all,
y = DataInt_training_all,
method = "rf",
metric = "MSE",
trControl = rfControl,
ntree = ntree,
tuneGrid = data.frame(mtry = mtry_fixed))
cat("Initial training completed.\n")
# Save the initial model
saveRDS(clf_rf_all, file = paste0("clf_rf_all_i1_seed", seed_i, ".rds"))
# Extract and print importance
importance_all <- as.data.frame(importance(clf_rf_all$finalModel))
print(importance_all)
}, finally = {
# Ensure the cluster is stopped, and temp file moved back after the process
try(stopAllClusters(), silent = TRUE)
files_to_load <- list.files("temp", full.names = TRUE)
for (file in files_to_load) {
load(file)
}
# Delete everything in the 'temp' folder
file.remove(files_to_load)
})
source("C:/Users/zhenw/Marc Hellerstein Lab/Orbitrap_Paper/2cRF_Bias_Correction_by_RandomForest/2cRF_Bias_Correction_by_RandomForest_20250801_bugfixed.R", echo=TRUE)
source("C:/Users/zhenw/Marc Hellerstein Lab/Orbitrap_Paper/2cRF_Bias_Correction_by_RandomForest/2cRF_Bias_Correction_by_RandomForest_20250801_bugfixed.R", echo=TRUE)
timestamp <- format(Sys.time(), "%Y%m%d_%H%M%S")
timestamp
final_log_file <- paste0("output_log_", timestamp, "_2c.txt")
###################################
# File: 2cRF_Bias_Correction_by_RandomForest
# Input: "All_1c_Combined_2a.xlsx" is the output of step 2a, which is all combined data of peak and by slice infor extracted in step 1.
# Function: Filter the data to remove any extreme outliers (which is likely due to misidentifidation);
#         Calculaten some more features that will be used in step 2c model training;
#         Inspect the input data structure to determine an intensity cutoff to know which signals could be considered
# Output: Two Rdata files: "df_by_mn_high_2b.RData" and "df_by_mn_low_2b.RData" , which contains data above and below the determined intensity cutoff
# Usage: Before running, copy the 2a.xlsx file to the same folder as this 2b codes. During running, keep an eye on the plot and final summary
#       to check if the results make sense
# Next to Do: Copy the two Rdata "df_by_mn_high_2b.RData" and "df_by_mn_low_2b.RData"to 2cRF and 2cEN folders' "input"  folder to do actual model training
#
# Disclaimer: Chatgpt is used to debug this code.
####################################
#####################################
library(readxl) #load xlsx
library(purrr) #load xlsx by iterations
library(openxlsx) #write xlsx
library(magrittr) #pipe
library(tidyr) #df cleaning
library(plyr)  #df splitting and combining
library(dplyr) #df data manipulation
library(caret) #training
library(randomForest) #rf related plot
library(data.table) # handle large data without using too much RAM
library(tidyverse) # ggplot2, dplyr and more
library(ggplot2)
library(writexl)
library(gridExtra) # enable displaying multiple ggplot2 in one page
directory <- getwd()
source("2c_Functions.R") # custom functions
# seed_i is the random seed used to separate 80% training and 20% test data by sample
seed_i <- 2025
# total number of model training reruns to check if the result is robust, in each rerun, the used seed will +1 from the previous
num_rerun <- 1
# inner_cv is the within training cross-validation fold, 3 is chose to speed up training
inner_cv <- 3
# Recommended ntree for this data size
ntree <- 2000  # Adjust as necessary, 2000 is a good one for this study but be careful to not over RAM if your computer is not good. You may try differnt values for optimization
mtry_fixed <- 5 # A common starting point for mtry(tree depth) in RF
write("", file = "output_log.txt")
log_file <- "output_log.txt"
sink(log_file, append = TRUE)
current_time <- format(Sys.time(), "%Y%m%d_%H%M%S")
log_file_name <- sprintf("output_log_seed%d_rerun%d_time%s.txt", seed_i, num_rerun, current_time)
write("", file = log_file_name)
sink(log_file_name, append = TRUE)
source("C:/Users/zhenw/Marc Hellerstein Lab/Orbitrap_Paper/2cRF_Bias_Correction_by_RandomForest/2cRF_Bias_Correction_by_RandomForest_ver20250804.R", echo=TRUE)
source("C:/Users/zhenw/Marc Hellerstein Lab/Orbitrap_Paper/2cRF_Bias_Correction_by_RandomForest/2cRF_Bias_Correction_by_RandomForest_ver20250804.R", echo=TRUE)
source("C:/Users/zhenw/Marc Hellerstein Lab/Orbitrap_Paper/2cRF_Bias_Correction_by_RandomForest/2cRF_Bias_Correction_by_RandomForest_ver20250804.R", echo=TRUE)
source("C:/Users/zhenw/Marc Hellerstein Lab/Orbitrap_Paper/2cRF_Bias_Correction_by_RandomForest/2cRF_Bias_Correction_by_RandomForest_ver20250804.R", echo=TRUE)
source("C:/Users/zhenw/Marc Hellerstein Lab/Orbitrap_Paper/2cRF_Bias_Correction_by_RandomForest/2cRF_Bias_Correction_by_RandomForest_ver20250804.R", echo=TRUE)
View(df_by_mn_high)
seq(5, length(unique_values), by=5)
samples[seq(5, length(samples), by = 5)]
predictions_high_training
View(training_all_info)
